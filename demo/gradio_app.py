"""
Gradio Demo for Product AI Chat service (Gradio 6.x compatible).

Run with:
    python demo/gradio_app.py
"""
from __future__ import annotations

import json
import logging
import os
import sys
import threading
from collections import deque
from datetime import datetime
from typing import Any

import gradio as gr
import httpx

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# API base URL
API_BASE = os.getenv("API_BASE_URL", "http://127.0.0.1:8000")
API_TIMEOUT = 30.0


# =============================================================================
# Helpers
# =============================================================================
def _env_bool(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    return raw.strip().lower() in {"1", "true", "yes", "y", "on"}


def _parse_gradio_auth(value: str | None) -> tuple[str, str] | list[tuple[str, str]] | None:
    """
    Parse GRADIO_AUTH env var.

    Formats:
      - "user:pass"
      - "user:pass,user2:pass2"
    """
    if not value or not value.strip():
        return None
    pairs: list[tuple[str, str]] = []
    for item in value.split(","):
        item = item.strip()
        if not item:
            continue
        if ":" not in item:
            continue
        user, pwd = item.split(":", 1)
        user = user.strip()
        pwd = pwd.strip()
        if user and pwd:
            pairs.append((user, pwd))
    if not pairs:
        return None
    return pairs[0] if len(pairs) == 1 else pairs


# =============================================================================
# Log Collector - captures logs from the backend
# =============================================================================
class LogCollector:
    """Collects logs from API responses and stores them for display."""
    
    def __init__(self, max_entries: int = 500):
        self._entries: deque[dict] = deque(maxlen=max_entries)
        self._lock = threading.Lock()
    
    def add_entry(
        self,
        level: str,
        source: str,
        message: str,
        trace_id: str | None = None,
        extra: dict | None = None,
    ) -> None:
        """Add a log entry."""
        with self._lock:
            self._entries.append({
                "timestamp": datetime.now().strftime("%H:%M:%S.%f")[:-3],
                "level": level,
                "source": source,
                "message": message,
                "trace_id": trace_id or "-",
                "extra": extra or {},
            })
    
    def add_request(self, endpoint: str, payload: dict | None = None) -> None:
        """Log an outgoing request."""
        # Extract message for display
        message_preview = ""
        if payload:
            msg = payload.get("message", "")
            if msg:
                message_preview = f" | msg: \"{msg[:50]}{'...' if len(msg) > 50 else ''}\""
            product_id = payload.get("product_id", "")
            if product_id:
                message_preview += f" | product: {product_id[:20]}..."
        
        self.add_entry(
            level="REQUEST",
            source="gradio",
            message=f"‚Üí {endpoint}{message_preview}",
            extra={"payload": payload},
        )
    
    def add_response(self, endpoint: str, response: dict, trace_id: str | None = None) -> None:
        """Log a response and extract debug info."""
        # Extract debug info from response
        meta = response.get("meta", {})
        debug = meta.get("debug", {})
        
        # Extract reply text preview
        reply = response.get("reply", {})
        reply_text = reply.get("text", "") if isinstance(reply, dict) else ""
        reply_preview = f" | reply: \"{reply_text[:60]}{'...' if len(reply_text) > 60 else ''}\"" if reply_text else ""
        
        # Confidence
        confidence = meta.get("confidence", debug.get("confidence", 0))
        conf_str = f" | conf: {confidence:.0%}" if confidence else ""
        
        # Log main response info
        self.add_entry(
            level="RESPONSE",
            source="api",
            message=f"‚Üê {endpoint}{conf_str}{reply_preview}",
            trace_id=trace_id or debug.get("trace_id"),
            extra={"debug": debug},
        )
        
        # Extract and log pipeline details
        if debug:
            self._log_debug_details(debug)
    
    def _log_debug_details(self, debug: dict) -> None:
        """Extract meaningful info from debug payload."""
        trace_id = debug.get("trace_id", "-")
        
        # Detect Product Chat vs Main Chat by checking for product_id in debug
        is_product_chat = "product_id" in debug or "context_hash" in debug
        
        if is_product_chat:
            self._log_product_chat_debug(debug, trace_id)
        else:
            self._log_main_chat_debug(debug, trace_id)
    
    def _log_product_chat_debug(self, debug: dict, trace_id: str) -> None:
        """Log Product Chat specific debug info."""
        product_id = debug.get("product_id", "-")
        llm_used = debug.get("llm_used", False)
        llm_cached = debug.get("llm_cached", False)
        model = debug.get("model", "unknown")
        
        # Context info
        context_cache_hit = debug.get("context_cache_hit", False)
        context_hash = debug.get("context_hash", "")[:12] if debug.get("context_hash") else "-"
        
        self.add_entry(
            level="CONTEXT",
            source="product_gateway",
            message=f"Product: {product_id[:20]}... | Context hash: {context_hash} | Cache: {'‚úì' if context_cache_hit else '‚úó'}",
            trace_id=trace_id,
        )
        
        # Policy check
        out_of_scope = debug.get("out_of_scope", False)
        injection_detected = debug.get("injection_detected", False)
        refusal_reason = debug.get("refusal_reason")
        
        if out_of_scope or injection_detected or refusal_reason:
            self.add_entry(
                level="POLICY",
                source="policy_guard",
                message=f"Out of scope: {'‚úì' if out_of_scope else '‚úó'} | Injection: {'‚úì' if injection_detected else '‚úó'} | Refusal: {refusal_reason or 'none'}",
                trace_id=trace_id,
            )
        
        # LLM details
        if llm_used:
            token_usage = debug.get("token_usage", {})
            prompt_tokens = token_usage.get("prompt_tokens", 0)
            completion_tokens = token_usage.get("completion_tokens", 0)
            
            self.add_entry(
                level="LLM",
                source="llm",
                message=f"Model: {model} | Cached: {'‚úì' if llm_cached else '‚úó'} | Tokens: {prompt_tokens}‚Üí{completion_tokens}",
                trace_id=trace_id,
            )
        
        # Used fields (citations)
        used_fields = debug.get("used_fields", [])
        if used_fields:
            self.add_entry(
                level="CITATIONS",
                source="llm",
                message=f"Used fields: {', '.join(used_fields[:10])}" + (f" (+{len(used_fields)-10} more)" if len(used_fields) > 10 else ""),
                trace_id=trace_id,
            )
    
    def _log_main_chat_debug(self, debug: dict, trace_id: str) -> None:
        """Log Main Chat specific debug info."""
        # Pipeline path
        pipeline = debug.get("pipeline_path", "unknown")
        router_matched = debug.get("router_matched", False)
        llm_used = debug.get("llm_used", False)
        
        self.add_entry(
            level="PIPELINE",
            source="backend",
            message=f"Pipeline: {pipeline} | Router: {'‚úì' if router_matched else '‚úó'} | LLM: {'‚úì' if llm_used else '‚úó'}",
            trace_id=trace_id,
        )
        
        # Intent chain
        intent_chain = debug.get("intent_chain", [])
        if intent_chain:
            self.add_entry(
                level="INTENT",
                source="nlu",
                message=f"Intents: {' ‚Üí '.join(intent_chain)}",
                trace_id=trace_id,
            )
        
        # Router details
        if router_matched:
            confidence = debug.get("router_confidence", 0)
            match_type = debug.get("router_match_type", "unknown")
            triggers = debug.get("matched_triggers", [])
            self.add_entry(
                level="ROUTER",
                source="router",
                message=f"Match: {match_type} | Confidence: {confidence:.2f} | Triggers: {triggers[:5]}",
                trace_id=trace_id,
            )
        
        # LLM details
        if llm_used:
            llm_confidence = debug.get("llm_confidence", 0)
            llm_cached = debug.get("llm_cached", False)
            reasoning = debug.get("llm_reasoning", "")
            self.add_entry(
                level="LLM",
                source="llm",
                message=f"Confidence: {llm_confidence:.2f} | Cached: {'‚úì' if llm_cached else '‚úó'}" + 
                        (f" | Reasoning: {reasoning[:100]}..." if reasoning else ""),
                trace_id=trace_id,
            )
        
        # Extracted entities
        entities = debug.get("extracted_entities", {})
        if entities:
            self.add_entry(
                level="SLOTS",
                source="extraction",
                message=f"Extracted: {entities}",
                trace_id=trace_id,
            )
        
        # Slot filling
        if debug.get("slot_filling_used"):
            missing = debug.get("missing_slots", [])
            filled = debug.get("filled_slots", [])
            self.add_entry(
                level="SLOTS",
                source="slot_manager",
                message=f"Filled: {filled} | Missing: {missing}",
                trace_id=trace_id,
            )
    
    def get_entries(self, limit: int = 100) -> list[dict]:
        """Get recent log entries."""
        with self._lock:
            entries = list(self._entries)
            return entries[-limit:]
    
    def clear(self) -> None:
        """Clear all entries."""
        with self._lock:
            self._entries.clear()
    
    def format_as_text(self, limit: int = 100, filter_level: str | None = None) -> str:
        """Format entries as readable text."""
        entries = self.get_entries(limit)
        
        if filter_level and filter_level != "ALL":
            entries = [e for e in entries if e["level"] == filter_level]
        
        if not entries:
            return "üì≠ –õ–æ–≥–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–ø—Ä–æ—Å –≤ –ª—é–±—É—é –≤–∫–ª–∞–¥–∫—É."
        
        lines = []
        for entry in entries:
            level_emoji = {
                "REQUEST": "üì§",
                "RESPONSE": "üì•",
                "PIPELINE": "üîÄ",
                "INTENT": "üéØ",
                "CONTEXT": "üìã",
                "POLICY": "üõ°Ô∏è",
                "CITATIONS": "üìé",
                "ROUTER": "üîç",
                "LLM": "ü§ñ",
                "SLOTS": "üì¶",
                "ERROR": "‚ùå",
                "INFO": "‚ÑπÔ∏è",
            }.get(entry["level"], "‚Ä¢")
            
            line = f"`{entry['timestamp']}` {level_emoji} **{entry['level']}** [{entry['source']}]"
            if entry["trace_id"] != "-":
                line += f" `{entry['trace_id'][:8]}`"
            line += f"\n{entry['message']}"
            
            lines.append(line)
        
        return "\n\n---\n\n".join(lines)
    
    def format_as_json(self, limit: int = 50) -> str:
        """Format entries as JSON for debugging."""
        entries = self.get_entries(limit)
        return json.dumps(entries, ensure_ascii=False, indent=2)


# Global log collector instance
log_collector = LogCollector()


def api_call(method: str, endpoint: str, json_data: dict | None = None) -> dict:
    """Make API call to our backend with logging."""
    url = f"{API_BASE}{endpoint}"
    
    # Log the request
    log_collector.add_request(endpoint, json_data)
    
    try:
        with httpx.Client(timeout=API_TIMEOUT) as client:
            if method == "GET":
                resp = client.get(url)
            elif method == "DELETE":
                resp = client.delete(url)
            else:
                resp = client.post(url, json=json_data)
            resp.raise_for_status()
            result = resp.json()
            
            # Log the response with debug info extraction
            trace_id = result.get("meta", {}).get("debug", {}).get("trace_id")
            log_collector.add_response(endpoint, result, trace_id)
            
            return result
    except httpx.HTTPStatusError as e:
        error_result = {"error": f"HTTP {e.response.status_code}", "detail": e.response.text}
        log_collector.add_entry(
            level="ERROR",
            source="api",
            message=f"HTTP Error {e.response.status_code}: {e.response.text[:200]}",
        )
        return error_result
    except httpx.RequestError as e:
        error_result = {"error": "Connection Error", "detail": str(e)}
        log_collector.add_entry(
            level="ERROR",
            source="api",
            message=f"Connection Error: {str(e)}",
        )
        return error_result


# =============================================================================
# Tab 0: Main Assistant Chat
# =============================================================================
def chat_with_assistant(
    message: str,
    history: list[dict],
    user_id: str,
    conversation_id: str,
) -> tuple[list[dict], str, str]:
    """Send message to Main AI Assistant. Returns updated history, conversation_id, and debug info."""
    if not message.strip():
        return history, conversation_id, ""

    payload = {
        "message": message.strip(),
        "user_id": user_id.strip() if user_id.strip() else "demo-user",
    }
    if conversation_id.strip():
        payload["conversation_id"] = conversation_id.strip()

    result = api_call("POST", "/api/ai/chat/message", payload)

    if "error" in result:
        bot_msg = f"‚ùå {result['error']}: {result.get('detail', '')}"
        debug_info = ""
    else:
        reply = result.get("reply", {})
        text = reply.get("text", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
        
        # Format response with additional info
        bot_msg = text
        
        # Add product cards if available
        data = result.get("data", {})
        products = data.get("products", [])
        if products:
            bot_msg += f"\n\nüì¶ –ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}"
            for p in products[:3]:
                name = p.get("name", p.get("title", "–¢–æ–≤–∞—Ä"))
                price = p.get("price", "")
                bot_msg += f"\n‚Ä¢ {name}" + (f" ‚Äî {price}‚ÇΩ" if price else "")
            if len(products) > 3:
                bot_msg += f"\n‚Ä¢ ... –∏ –µ—â—ë {len(products) - 3}"
        
        # Add quick replies if available
        meta = result.get("meta", {})
        quick_replies = meta.get("quick_replies", [])
        if quick_replies:
            bot_msg += "\n\nüí° " + " | ".join(quick_replies[:4])
        
        # Update conversation_id from response
        conversation_id = result.get("conversation_id", conversation_id)
        
        # Format debug info
        debug = meta.get("debug", {})
        debug_info = _format_debug_summary(debug)

    history = history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": bot_msg},
    ]
    return history, conversation_id, debug_info


def _format_debug_summary(debug: dict) -> str:
    """Format debug info as a compact summary."""
    if not debug:
        return ""
    
    lines = []
    
    # Pipeline
    pipeline = debug.get("pipeline_path", "unknown")
    lines.append(f"**Pipeline:** `{pipeline}`")
    
    # Intent
    intents = debug.get("intent_chain", [])
    if intents:
        lines.append(f"**Intent:** `{intents[-1]}`")
    
    # Router
    if debug.get("router_matched"):
        conf = debug.get("router_confidence", 0)
        match_type = debug.get("router_match_type", "")
        lines.append(f"**Router:** ‚úì {match_type} ({conf:.0%})")
    
    # LLM
    if debug.get("llm_used"):
        cached = "cached" if debug.get("llm_cached") else "fresh"
        llm_conf = debug.get("llm_confidence", 0)
        lines.append(f"**LLM:** ‚úì {cached} ({llm_conf:.0%})")
    
    # Slots
    entities = debug.get("extracted_entities", {})
    if entities:
        slots_str = ", ".join(f"{k}={v}" for k, v in list(entities.items())[:3])
        lines.append(f"**Slots:** {slots_str}")
    
    # Trace ID
    trace_id = debug.get("trace_id", "")
    if trace_id:
        lines.append(f"**Trace:** `{trace_id[:12]}...`")
    
    return "\n".join(lines)


# =============================================================================
# Tab 1: Product Chat (Gradio 6.x format)
# =============================================================================
def init_product_chat(product_id: str, store_id: str) -> tuple[list[dict], str]:
    """Initialize product chat with greeting and AI summary."""
    payload = {
        "product_id": product_id.strip(),
        "store_id": store_id.strip() if store_id.strip() else None,
        "shipping_method": "PICKUP",
    }
    
    result = api_call("POST", "/api/product-ai/chat/init", payload)
    
    if "error" in result:
        return [], ""
    
    conversation_id = result.get("conversation_id", "")
    greeting = result.get("greeting", "")
    ai_summary = result.get("ai_summary")
    
    # Build greeting message (without suggested questions - we have buttons for that)
    greeting_parts = [greeting]
    
    if ai_summary:
        greeting_parts.append(f"\n\nüìù **AI-–æ–±–∑–æ—Ä —Ç–æ–≤–∞—Ä–∞:**\n{ai_summary}")
    
    greeting_parts.append("\n\nüëÜ *–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–Ω–æ–ø–∫–∏ –±—ã—Å—Ç—Ä—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å*")
    
    greeting_message = "".join(greeting_parts)
    
    history = [{"role": "assistant", "content": greeting_message}]
    return history, conversation_id


def chat_with_product(
    message: str,
    history: list[dict],
    product_id: str,
    store_id: str,
    conversation_id: str,
) -> tuple[list[dict], str]:
    """Send message to Product AI Chat. Returns updated history and conversation_id."""
    if not product_id.strip():
        history = history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": "‚ùå –û—à–∏–±–∫–∞: —É–∫–∞–∂–∏—Ç–µ Product ID"},
        ]
        return history, conversation_id

    if not message.strip():
        return history, conversation_id

    # Initialize chat on first message (empty conversation_id)
    if not conversation_id.strip():
        init_history, conversation_id = init_product_chat(product_id, store_id)
        if init_history:
            history = init_history

    payload = {
        "product_id": product_id.strip(),
        "message": message.strip(),
        "store_id": store_id.strip() if store_id.strip() else None,
        "shipping_method": "PICKUP",
        "conversation_id": conversation_id,
    }

    result = api_call("POST", "/api/product-ai/chat/message", payload)

    if "error" in result:
        bot_msg = f"‚ùå {result['error']}: {result.get('detail', '')}"
    else:
        reply = result.get("reply", {})
        text = reply.get("text", "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞")
        citations = result.get("citations", [])
        
        bot_msg = text
        if citations:
            # citations is a list of dicts with field_path
            citation_fields = [c.get("field_path", str(c)) for c in citations]
            bot_msg += f"\n\nüìé –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(citation_fields)}"
        
        # Update conversation_id from response
        conversation_id = result.get("conversation_id", conversation_id)

    history = history + [
        {"role": "user", "content": message},
        {"role": "assistant", "content": bot_msg},
    ]
    return history, conversation_id


# =============================================================================
# Tab 2: FAQ Generator
# =============================================================================
def generate_faq(product_id: str, force_refresh: bool) -> str:
    """Generate FAQ for a product."""
    if not product_id.strip():
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ Product ID"

    endpoint = f"/api/product-ai/faq/{product_id.strip()}"
    if force_refresh:
        endpoint += "?force_refresh=true"

    result = api_call("GET", endpoint)

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    output = f"## FAQ –¥–ª—è —Ç–æ–≤–∞—Ä–∞: {result.get('product_name', product_id)}\n\n"
    
    for i, faq in enumerate(result.get("faqs", []), 1):
        output += f"### {i}. {faq['question']}\n"
        output += f"{faq['answer']}\n"
        if faq.get("used_fields"):
            output += f"*–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(faq['used_fields'])}*\n"
        output += "\n"

    meta = result.get("meta", {})
    output += f"\n---\n*–ò—Å—Ç–æ—á–Ω–∏–∫: {meta.get('source', 'unknown')} | "
    output += f"–ö—ç—à: {'–¥–∞' if result.get('cache_hit') else '–Ω–µ—Ç'}*"

    return output


# =============================================================================
# Tab 3: Drug Interactions
# =============================================================================
def check_drug_interactions(drugs_text: str) -> str:
    """Check drug interactions."""
    drugs = [d.strip() for d in drugs_text.split(",") if d.strip()]
    
    if len(drugs) < 2:
        return "‚ùå –í–≤–µ–¥–∏—Ç–µ –º–∏–Ω–∏–º—É–º 2 –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é"

    result = api_call("POST", "/api/product-ai/drug-interactions/check", {
        "drugs": drugs
    })

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    if not result.get("has_interactions"):
        return f"‚úÖ –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ\n\n–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ: {', '.join(drugs)}"

    output = "## ‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n\n"
    
    for interaction in result.get("interactions", []):
        severity = interaction.get("severity", "unknown")
        emoji = {"critical": "üî¥", "major": "üü†", "moderate": "üü°", "minor": "üü¢"}.get(severity, "‚ö™")
        
        output += f"### {emoji} {interaction['drug_a']} + {interaction['drug_b']}\n"
        output += f"**–£—Ä–æ–≤–µ–Ω—å:** {severity}\n\n"
        output += f"{interaction.get('description', '')}\n\n"
        if interaction.get("recommendation"):
            output += f"**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {interaction['recommendation']}\n\n"
        output += "---\n\n"

    return output


# =============================================================================
# Tab 4: Smart Analogs
# =============================================================================
def find_analogs(
    product_id: str,
    inn: str,
    current_price: float,
    limit: int,
    only_cheaper: bool,
) -> str:
    """Find product analogs by INN."""
    if not inn.strip():
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ –ú–ù–ù (–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ –≤–µ—â–µ—Å—Ç–≤–æ)"

    payload = {
        "product_id": product_id.strip() or "unknown",
        "inn": inn.strip(),
        "current_price": current_price if current_price > 0 else None,
        "limit": int(limit),
        "only_cheaper": only_cheaper,
    }

    result = api_call("POST", "/api/product-ai/analogs/find", payload)

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    analogs = result.get("analogs", [])
    if not analogs:
        return f"–ê–Ω–∞–ª–æ–≥–æ–≤ —Å –ú–ù–ù '{inn}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"

    output = f"## –ê–Ω–∞–ª–æ–≥–∏ –ø–æ –ú–ù–ù: {result.get('inn', inn)}\n\n"
    output += f"–ù–∞–π–¥–µ–Ω–æ: {result.get('total_found', len(analogs))}\n\n"

    for i, analog in enumerate(analogs, 1):
        saving = analog.get("savings_amount")
        saving_text = f" (—ç–∫–æ–Ω–æ–º–∏—è {saving:.0f}‚ÇΩ)" if saving and saving > 0 else ""
        
        output += f"### {i}. {analog['name']}\n"
        output += f"- **–¶–µ–Ω–∞:** {analog['price']:.0f}‚ÇΩ{saving_text}\n"
        output += f"- **–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å:** {analog.get('manufacturer', '–Ω/–¥')}\n"
        output += f"- **–§–æ—Ä–º–∞:** {analog.get('form', '–Ω/–¥')}\n"
        if analog.get("in_stock"):
            output += "- ‚úÖ –í –Ω–∞–ª–∏—á–∏–∏\n"
        output += "\n"

    return output


# =============================================================================
# Tab 5: Course Calculator
# =============================================================================
def calculate_course(
    product_id: str,
    units_per_package: int,
    dose_per_intake: int,
    frequency: str,
    course_days: int,
    reserve_percent: int,
    price_per_package: float,
) -> str:
    """Calculate medication course requirements."""
    if units_per_package <= 0:
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü –≤ —É–ø–∞–∫–æ–≤–∫–µ"

    payload = {
        "product_id": product_id.strip() or "product",
        "units_per_package": int(units_per_package),
        "dose_per_intake": int(dose_per_intake),
        "frequency": frequency,
        "course_days": int(course_days),
        "add_reserve_percent": int(reserve_percent),
    }

    result = api_call("POST", "/api/product-ai/course/calculate", payload)

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    output = "## üìä –†–∞—Å—á—ë—Ç –∫—É—Ä—Å–∞\n\n"
    output += f"### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n"
    output += f"- –ï–¥–∏–Ω–∏—Ü –≤ —É–ø–∞–∫–æ–≤–∫–µ: **{result['units_per_package']}**\n"
    output += f"- –î–æ–∑–∞ –∑–∞ –ø—Ä–∏—ë–º: **{result['dose_per_intake']}** –µ–¥.\n"
    output += f"- –ß–∞—Å—Ç–æ—Ç–∞: **{result['intakes_per_day']}** —Ä–∞–∑/–¥–µ–Ω—å\n"
    output += f"- –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫—É—Ä—Å–∞: **{result['course_days']}** –¥–Ω–µ–π\n\n"

    output += f"### –†–µ–∑—É–ª—å—Ç–∞—Ç\n"
    output += f"- –í—Å–µ–≥–æ –Ω—É–∂–Ω–æ –µ–¥–∏–Ω–∏—Ü: **{result['total_units_needed']}**\n"
    output += f"- –£–ø–∞–∫–æ–≤–æ–∫ –Ω—É–∂–Ω–æ: **{result['packages_needed']}** —à—Ç.\n"
    output += f"- –û—Å—Ç–∞–Ω–µ—Ç—Å—è –µ–¥–∏–Ω–∏—Ü: **{result['units_remaining']}**\n"

    if result.get("packages_with_reserve"):
        output += f"\n### –° –∑–∞–ø–∞—Å–æ–º ({result['reserve_percent']}%)\n"
        output += f"- –£–ø–∞–∫–æ–≤–æ–∫ —Å –∑–∞–ø–∞—Å–æ–º: **{result['packages_with_reserve']}** —à—Ç.\n"

    if price_per_package > 0:
        total = result['packages_needed'] * price_per_package
        output += f"\n### –°—Ç–æ–∏–º–æ—Å—Ç—å\n"
        output += f"- –¶–µ–Ω–∞ –∑–∞ —É–ø–∞–∫–æ–≤–∫—É: {price_per_package:.0f}‚ÇΩ\n"
        output += f"- **–ò—Ç–æ–≥–æ: {total:.0f}‚ÇΩ**\n"

    output += f"\n---\n*{result.get('recommendation', '')}*"

    return output


# =============================================================================
# Tab 6: Personalization
# =============================================================================
def get_personalization(user_id: str, product_id: str) -> str:
    """Get personalization context for user."""
    if not user_id.strip():
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ User ID"

    payload = {
        "user_id": user_id.strip(),
        "product_id": product_id.strip() or None,
    }

    result = api_call("POST", "/api/product-ai/personalization/context", payload)

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    ctx = result.get("context", {})
    profile = result.get("profile", {})

    output = f"## üë§ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è {user_id}\n\n"
    
    output += "### –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n"
    output += f"- –°—Ç–∞—Ç—É—Å: **{'–í–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π—Å—è' if ctx.get('is_returning_user') else '–ù–æ–≤—ã–π'}** –ø–æ–∫—É–ø–∞—Ç–µ–ª—å\n"
    output += f"- –í—Å–µ–≥–æ –ø–æ–∫—É–ø–æ–∫: **{profile.get('total_purchases', 0)}**\n"
    output += f"- –û–±—â–∞—è —Å—É–º–º–∞: **{profile.get('total_spent', 0):.0f}‚ÇΩ**\n"
    
    if profile.get("favorite_categories"):
        output += f"- –õ—é–±–∏–º—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(profile['favorite_categories'])}\n"

    if ctx.get("personalized_greeting"):
        output += f"\n### –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ\n> {ctx['personalized_greeting']}\n"

    if ctx.get("suggested_quantity"):
        output += f"\n### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n"
        output += f"- –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: **{ctx['suggested_quantity']}** —à—Ç.\n"

    if result.get("also_bought"):
        output += f"\n### –° —ç—Ç–∏–º —Ç–æ–≤–∞—Ä–æ–º –ø–æ–∫—É–ø–∞–ª–∏\n"
        for item in result["also_bought"][:5]:
            output += f"- {item.get('name', item.get('product_id'))}\n"

    return output


# =============================================================================
# Tab 7: Proactive Hints
# =============================================================================
def get_proactive_hints(product_id: str, trigger_type: str, limit: int) -> str:
    """Get proactive hints for product."""
    if not product_id.strip():
        return "‚ùå –£–∫–∞–∂–∏—Ç–µ Product ID"

    payload = {
        "product_id": product_id.strip(),
        "trigger_type": trigger_type,
        "limit": int(limit),
    }

    result = api_call("POST", "/api/product-ai/proactive/hints", payload)

    if "error" in result:
        return f"‚ùå {result['error']}: {result.get('detail', '')}"

    hints = result.get("hints", [])
    if not hints:
        return "–ü–æ–¥—Å–∫–∞–∑–æ–∫ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç—Ä–∏–≥–≥–µ—Ä–∞ –Ω–µ—Ç"

    output = f"## üí° –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏\n\n"
    output += f"**–¢—Ä–∏–≥–≥–µ—Ä:** {trigger_type}\n\n"

    for hint in hints:
        priority = hint.get("priority", 0)
        emoji = "üî•" if priority >= 9 else "‚≠ê" if priority >= 7 else "üí¨"
        
        output += f"### {emoji} {hint.get('hint_type', 'hint')}\n"
        output += f"{hint.get('message', '')}\n"
        
        if hint.get("suggested_question"):
            output += f"\n*–ü—Ä–µ–¥–ª–æ–∂–∏—Ç—å –≤–æ–ø—Ä–æ—Å:* \"{hint['suggested_question']}\"\n"
        
        output += f"\n*–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority}/10*\n\n"
        output += "---\n\n"

    return output


# =============================================================================
# Custom Theme & CSS for Professional Look
# =============================================================================
CUSTOM_CSS = """
/* Calm, neutral color scheme */
:root {
    --primary-color: #475569;
    --primary-hover: #334155;
    --background-light: #f9fafb;
    --border-color: #e5e7eb;
    --text-primary: #1f2937;
    --text-secondary: #6b7280;
}

/* Hide Gradio footer */
footer {
    display: none !important;
}

/* Main container */
.gradio-container {
    max-width: 1100px !important;
    margin: 0 auto !important;
}

/* Tabs */
.tab-nav button {
    font-weight: 500 !important;
    padding: 0.5rem 1rem !important;
}

/* Chat */
.chatbot {
    border: 1px solid var(--border-color) !important;
    border-radius: 8px !important;
}

/* Chat messages - better readability */
.chatbot .message {
    font-size: 0.9rem !important;
    line-height: 1.6 !important;
    padding: 0.75rem 1rem !important;
    margin-bottom: 0.5rem !important;
}

.chatbot .bot {
    background: #f9fafb !important;
}

.chatbot .user {
    background: #475569 !important;
    color: white !important;
}

/* Buttons - calm style */
button.primary {
    background: #475569 !important;
    border: none !important;
    border-radius: 6px !important;
    font-weight: 500 !important;
}

button.primary:hover {
    background: #334155 !important;
}

button.secondary {
    background: #f9fafb !important;
    border: 1px solid #d1d5db !important;
    color: #374151 !important;
    border-radius: 6px !important;
    font-weight: 400 !important;
}

button.secondary:hover {
    background: #f3f4f6 !important;
    border-color: #9ca3af !important;
}

/* Inputs */
textarea, input[type="text"] {
    border: 1px solid var(--border-color) !important;
    border-radius: 6px !important;
}

textarea:focus, input[type="text"]:focus {
    border-color: #9ca3af !important;
    box-shadow: 0 0 0 2px rgba(156, 163, 175, 0.15) !important;
}

/* Quick buttons row - equal spacing */
.row {
    gap: 0.5rem !important;
}
"""


# Professional theme (Gradio 6.x - passed to launch())
# Calm, neutral colors
DEMO_THEME = gr.themes.Soft(
    primary_hue="slate",
    secondary_hue="slate",
    neutral_hue="slate",
    font=gr.themes.GoogleFont("Inter"),
).set(
    button_primary_background_fill="#475569",
    button_primary_background_fill_hover="#334155",
    block_radius="8px",
    input_radius="6px",
)


# =============================================================================
# Build Gradio Interface (Gradio 6.x)
# =============================================================================
def create_demo() -> gr.Blocks:
    """Create Gradio demo interface."""
    
    with gr.Blocks(title="Product AI Assistant | Demo") as demo:
        
        # Clean Header (white background)
        gr.HTML("""
            <div style="text-align: center; padding: 1.5rem 0; margin-bottom: 1rem; border-bottom: 1px solid #e5e7eb;">
                <h1 style="margin: 0; font-size: 1.5rem; font-weight: 600; color: #1f2937;">
                    Product AI Assistant
                </h1>
                <p style="margin: 0.25rem 0 0 0; font-size: 0.9rem; color: #6b7280;">
                    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –¥–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
                </p>
            </div>
        """)

        with gr.Tabs():
            # Tab 0: Product Chat (MAIN FEATURE - First!)
            with gr.TabItem("–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ —Ç–æ–≤–∞—Ä—É"):
                
                with gr.Row():
                    # Left sidebar - clean settings
                    with gr.Column(scale=1, min_width=240):
                        chat_product_id = gr.Textbox(
                            label="Product ID",
                            placeholder="UUID —Ç–æ–≤–∞—Ä–∞",
                            value="62eb2515-1608-4812-9caa-12ad48c975c5",
                        )
                        
                        chat_store_id = gr.Textbox(
                            label="Store ID (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                            placeholder="ID –º–∞–≥–∞–∑–∏–Ω–∞",
                        )
                        
                        chat_conversation_id = gr.State("")
                        chat_suggested_questions = gr.State([])
                        
                        start_chat_btn = gr.Button("–ù–∞—á–∞—Ç—å –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—é", variant="primary", size="lg")
                        clear_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å", variant="secondary")
                    
                    # Main chat area
                    with gr.Column(scale=3):
                        chatbot = gr.Chatbot(
                            label="–î–∏–∞–ª–æ–≥",
                            height=420,
                        )
                        
                        # Quick question buttons - 4 buttons to fit in one row
                        with gr.Row():
                            quick_q1 = gr.Button("–°–æ—Å—Ç–∞–≤", size="sm", variant="secondary")
                            quick_q2 = gr.Button("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", size="sm", variant="secondary")
                            quick_q3 = gr.Button("–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è", size="sm", variant="secondary")
                            quick_q4 = gr.Button("–ü–æ–±–æ—á–Ω—ã–µ", size="sm", variant="secondary")
                        with gr.Row():
                            quick_q5 = gr.Button("–†–µ—Ü–µ–ø—Ç", size="sm", variant="secondary")
                            quick_q6 = gr.Button("–ë–µ—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç—å", size="sm", variant="secondary")
                            quick_q7 = gr.Button("–•—Ä–∞–Ω–µ–Ω–∏–µ", size="sm", variant="secondary")
                            quick_q8 = gr.Button("–°—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏", size="sm", variant="secondary")
                        
                        with gr.Row():
                            chat_input = gr.Textbox(
                                label="",
                                placeholder="–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –æ —Ç–æ–≤–∞—Ä–µ...",
                                lines=1,
                                scale=5,
                                container=False,
                            )
                            chat_btn = gr.Button("‚Üí", variant="primary", scale=1, min_width=60)

                def handle_start_chat(product_id, store_id):
                    """Initialize chat and show greeting with AI summary."""
                    if not product_id.strip():
                        return [{"role": "assistant", "content": "‚ùå –£–∫–∞–∂–∏—Ç–µ Product ID"}], "", []
                    
                    history, conv_id = init_product_chat(product_id, store_id)
                    if not history:
                        return [{"role": "assistant", "content": "‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —á–∞—Ç–∞"}], "", []
                    
                    return history, conv_id, []

                def handle_chat(message, history, product_id, store_id, conv_id):
                    new_history, new_conv_id = chat_with_product(
                        message, history, product_id, store_id, conv_id
                    )
                    return new_history, "", new_conv_id

                def handle_quick_question(question, history, product_id, store_id, conv_id):
                    """Handle quick question button click."""
                    return handle_chat(question, history, product_id, store_id, conv_id)

                # Start chat button
                start_chat_btn.click(
                    handle_start_chat,
                    inputs=[chat_product_id, chat_store_id],
                    outputs=[chatbot, chat_conversation_id, chat_suggested_questions],
                )

                # Send message button
                chat_btn.click(
                    handle_chat,
                    inputs=[chat_input, chatbot, chat_product_id, chat_store_id, chat_conversation_id],
                    outputs=[chatbot, chat_input, chat_conversation_id],
                )
                
                # Quick question buttons
                for btn, question in [
                    (quick_q1, "–ö–∞–∫–æ–π —Å–æ—Å—Ç–∞–≤?"),
                    (quick_q2, "–ö–∞–∫ –ø—Ä–∏–Ω–∏–º–∞—Ç—å?"),
                    (quick_q3, "–ï—Å—Ç—å –ª–∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è?"),
                    (quick_q4, "–ö–∞–∫–∏–µ –ø–æ–±–æ—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã?"),
                    (quick_q5, "–ù—É–∂–µ–Ω –ª–∏ —Ä–µ—Ü–µ–ø—Ç?"),
                    (quick_q6, "–ú–æ–∂–Ω–æ –±–µ—Ä–µ–º–µ–Ω–Ω—ã–º?"),
                    (quick_q7, "–ö–∞–∫ —Ö—Ä–∞–Ω–∏—Ç—å?"),
                    (quick_q8, "–ö–∞–∫–æ–π —Å—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏?"),
                ]:
                    btn.click(
                        lambda h, p, s, c, q=question: handle_quick_question(q, h, p, s, c),
                        inputs=[chatbot, chat_product_id, chat_store_id, chat_conversation_id],
                        outputs=[chatbot, chat_input, chat_conversation_id],
                    )
                chat_input.submit(
                    handle_chat,
                    inputs=[chat_input, chatbot, chat_product_id, chat_store_id, chat_conversation_id],
                    outputs=[chatbot, chat_input, chat_conversation_id],
                )
                
                def do_clear():
                    return [], "", ""
                
                clear_btn.click(do_clear, outputs=[chatbot, chat_input, chat_conversation_id])

            # Tab 1: Main Assistant Chat (General pharmacy assistant)
            with gr.TabItem("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"):
                gr.Markdown("–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫: –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤, –∑–∞–∫–∞–∑—ã, –∫–æ—Ä–∑–∏–Ω–∞.")
                
                with gr.Row():
                    with gr.Column(scale=1):
                        main_user_id = gr.Textbox(
                            label="User ID",
                            placeholder="demo-user",
                            value="demo-user",
                        )
                        main_conversation_id = gr.State("")
                        main_clear_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å", variant="secondary")
                        
                        with gr.Accordion("Debug", open=False):
                            main_debug_output = gr.Markdown(
                                value="*–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ...*",
                            )
                    
                    with gr.Column(scale=3):
                        main_chatbot = gr.Chatbot(label="–î–∏–∞–ª–æ–≥", height=380)
                        
                        gr.Markdown("*–ü—Ä–∏–º–µ—Ä—ã: \"–±–æ–ª–∏—Ç –≥–æ–ª–æ–≤–∞\", \"–Ω–∞–π–¥–∏ –Ω—É—Ä–æ—Ñ–µ–Ω\", \"–ø–æ–∫–∞–∂–∏ –∫–æ—Ä–∑–∏–Ω—É\"*")
                        
                        with gr.Row():
                            main_input = gr.Textbox(
                                label="–í–∞—à –∑–∞–ø—Ä–æ—Å",
                                placeholder="–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å...",
                                lines=1,
                                scale=4,
                            )
                            main_chat_btn = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary", scale=1)

                def handle_main_chat(message, history, user_id, conv_id):
                    new_history, new_conv_id, debug_info = chat_with_assistant(
                        message, history, user_id, conv_id
                    )
                    return new_history, "", new_conv_id, debug_info or "*–ù–µ—Ç debug –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏*"

                main_chat_btn.click(
                    handle_main_chat,
                    inputs=[main_input, main_chatbot, main_user_id, main_conversation_id],
                    outputs=[main_chatbot, main_input, main_conversation_id, main_debug_output],
                )
                main_input.submit(
                    handle_main_chat,
                    inputs=[main_input, main_chatbot, main_user_id, main_conversation_id],
                    outputs=[main_chatbot, main_input, main_conversation_id, main_debug_output],
                )
                
                def do_main_clear():
                    return [], "", "", "*–ß–∞—Ç –æ—á–∏—â–µ–Ω*"
                
                main_clear_btn.click(
                    do_main_clear, 
                    outputs=[main_chatbot, main_input, main_conversation_id, main_debug_output]
                )

            # Tab 2: FAQ
            with gr.TabItem("üìã FAQ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä"):
                gr.Markdown("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è FAQ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞.")
                
                with gr.Row():
                    faq_product_id = gr.Textbox(
                        label="Product ID",
                        placeholder="12345",
                        value="12345",
                    )
                    faq_refresh = gr.Checkbox(label="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å", value=False)
                    faq_btn = gr.Button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å FAQ", variant="primary")
                
                faq_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                faq_btn.click(generate_faq, inputs=[faq_product_id, faq_refresh], outputs=[faq_output])

            # Tab 3: Drug Interactions
            with gr.TabItem("üíä –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è"):
                gr.Markdown("–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –º–µ–∂–¥—É –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞–º–∏.")
                
                drugs_input = gr.Textbox(
                    label="–ü—Ä–µ–ø–∞—Ä–∞—Ç—ã (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                    placeholder="–∏–±—É–ø—Ä–æ—Ñ–µ–Ω, –∞—Å–ø–∏—Ä–∏–Ω, –≤–∞—Ä—Ñ–∞—Ä–∏–Ω",
                    lines=2,
                )
                drugs_btn = gr.Button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å", variant="primary")
                drugs_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                drugs_btn.click(check_drug_interactions, inputs=[drugs_input], outputs=[drugs_output])

            # Tab 4: Analogs
            with gr.TabItem("üîÑ –ê–Ω–∞–ª–æ–≥–∏"):
                gr.Markdown("–ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –ø–æ –ú–ù–ù (–¥–µ–π—Å—Ç–≤—É—é—â–µ–º—É –≤–µ—â–µ—Å—Ç–≤—É).")
                
                with gr.Row():
                    analog_product_id = gr.Textbox(label="Product ID", placeholder="12345")
                    analog_inn = gr.Textbox(
                        label="–ú–ù–ù (–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ –≤–µ—â–µ—Å—Ç–≤–æ)",
                        placeholder="–∏–±—É–ø—Ä–æ—Ñ–µ–Ω",
                        value="–∏–±—É–ø—Ä–æ—Ñ–µ–Ω",
                    )
                
                with gr.Row():
                    analog_price = gr.Number(label="–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞ (‚ÇΩ)", value=350)
                    analog_limit = gr.Slider(label="–ú–∞–∫—Å. —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤", minimum=1, maximum=20, value=5, step=1)
                    analog_cheaper = gr.Checkbox(label="–¢–æ–ª—å–∫–æ –¥–µ—à–µ–≤–ª–µ", value=True)
                
                analog_btn = gr.Button("–ù–∞–π—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏", variant="primary")
                analog_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                analog_btn.click(
                    find_analogs,
                    inputs=[analog_product_id, analog_inn, analog_price, analog_limit, analog_cheaper],
                    outputs=[analog_output],
                )

            # Tab 5: Course Calculator
            with gr.TabItem("üìä –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –∫—É—Ä—Å–∞"):
                gr.Markdown("–†–∞—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–ø–∞–∫–æ–≤–æ–∫ –Ω–∞ –∫—É—Ä—Å –ª–µ—á–µ–Ω–∏—è.")
                
                with gr.Row():
                    course_product_id = gr.Textbox(label="Product ID", placeholder="12345")
                    course_units = gr.Number(label="–ï–¥–∏–Ω–∏—Ü –≤ —É–ø–∞–∫–æ–≤–∫–µ", value=30, precision=0)
                    course_dose = gr.Number(label="–î–æ–∑–∞ –∑–∞ –ø—Ä–∏—ë–º (–µ–¥.)", value=1, precision=0)
                
                with gr.Row():
                    course_frequency = gr.Dropdown(
                        label="–ß–∞—Å—Ç–æ—Ç–∞ –ø—Ä–∏—ë–º–∞",
                        choices=[
                            ("1 —Ä–∞–∑ –≤ –¥–µ–Ω—å", "once_daily"),
                            ("2 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å", "twice_daily"),
                            ("3 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å", "three_times_daily"),
                            ("4 —Ä–∞–∑–∞ –≤ –¥–µ–Ω—å", "four_times_daily"),
                            ("–ß–µ—Ä–µ–∑ –¥–µ–Ω—å", "every_other_day"),
                            ("1 —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é", "once_weekly"),
                        ],
                        value="once_daily",
                    )
                    course_days = gr.Number(label="–î–Ω–µ–π –∫—É—Ä—Å–∞", value=30, precision=0)
                    course_reserve = gr.Slider(label="–ó–∞–ø–∞—Å (%)", minimum=0, maximum=50, value=10, step=5)
                
                course_price = gr.Number(label="–¶–µ–Ω–∞ —É–ø–∞–∫–æ–≤–∫–∏ (‚ÇΩ, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)", value=0)
                course_btn = gr.Button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å", variant="primary")
                course_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                course_btn.click(
                    calculate_course,
                    inputs=[course_product_id, course_units, course_dose, course_frequency, 
                            course_days, course_reserve, course_price],
                    outputs=[course_output],
                )

            # Tab 6: Personalization
            with gr.TabItem("üë§ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è"):
                gr.Markdown("–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–∫—É–ø–æ–∫.")
                
                with gr.Row():
                    pers_user_id = gr.Textbox(
                        label="User ID",
                        placeholder="user-123",
                        value="user-123",
                    )
                    pers_product_id = gr.Textbox(
                        label="Product ID (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                        placeholder="12345",
                    )
                
                pers_btn = gr.Button("–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", variant="primary")
                pers_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                pers_btn.click(
                    get_personalization,
                    inputs=[pers_user_id, pers_product_id],
                    outputs=[pers_output],
                )

            # Tab 7: Proactive Hints
            with gr.TabItem("üí° –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏"):
                gr.Markdown("–ü–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")
                
                with gr.Row():
                    hints_product_id = gr.Textbox(
                        label="Product ID",
                        placeholder="12345",
                        value="12345",
                    )
                    hints_trigger = gr.Dropdown(
                        label="–¢—Ä–∏–≥–≥–µ—Ä",
                        choices=[
                            ("–í—Ä–µ–º—è –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ", "time_on_page"),
                            ("–ù–∞–º–µ—Ä–µ–Ω–∏–µ —É–π—Ç–∏", "exit_intent"),
                            ("–ì–ª—É–±–∏–Ω–∞ —Å–∫—Ä–æ–ª–ª–∞", "scroll_depth"),
                            ("–°–æ–º–Ω–µ–Ω–∏–µ —É –∫–æ—Ä–∑–∏–Ω—ã", "cart_hesitation"),
                            ("–ü–æ–≤—Ç–æ—Ä–Ω—ã–π –≤–∏–∑–∏—Ç", "return_visit"),
                        ],
                        value="time_on_page",
                    )
                    hints_limit = gr.Slider(label="–ú–∞–∫—Å. –ø–æ–¥—Å–∫–∞–∑–æ–∫", minimum=1, maximum=10, value=3, step=1)
                
                hints_btn = gr.Button("–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Å–∫–∞–∑–∫–∏", variant="primary")
                hints_output = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç")
                
                hints_btn.click(
                    get_proactive_hints,
                    inputs=[hints_product_id, hints_trigger, hints_limit],
                    outputs=[hints_output],
                )

            # Tab 8: Logs
            with gr.TabItem("üìä –õ–æ–≥–∏"):
                gr.Markdown(
                    """
                    ## –ñ—É—Ä–Ω–∞–ª –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    
                    –ó–¥–µ—Å—å –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –ª–æ–≥–∏ –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–∑ Gradio —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–∞–±–æ—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω–∞:
                    - **REQUEST/RESPONSE** ‚Äî –≤—Ö–æ–¥—è—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã API
                    - **PIPELINE** ‚Äî –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ (router_only, router+llm, llm_only –∏ —Ç.–¥.)
                    - **INTENT** ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã
                    - **ROUTER** ‚Äî –¥–µ—Ç–∞–ª–∏ –º–∞—Ç—á–∏–Ω–≥–∞ —Ä–æ—É—Ç–µ—Ä–∞ (—Ç—Ä–∏–≥–≥–µ—Ä—ã, confidence)
                    - **LLM** ‚Äî –¥–µ—Ç–∞–ª–∏ —Ä–∞–±–æ—Ç—ã LLM (confidence, reasoning)
                    - **SLOTS** ‚Äî –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
                    """
                )
                
                with gr.Row():
                    logs_filter = gr.Dropdown(
                        label="–§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É",
                        choices=[
                            ("–í—Å–µ", "ALL"),
                            ("–ó–∞–ø—Ä–æ—Å—ã/–æ—Ç–≤–µ—Ç—ã", "REQUEST"),
                            ("–ü–∞–π–ø–ª–∞–π–Ω", "PIPELINE"),
                            ("–ò–Ω—Ç–µ–Ω—Ç—ã", "INTENT"),
                            ("–†–æ—É—Ç–µ—Ä", "ROUTER"),
                            ("LLM", "LLM"),
                            ("–°–ª–æ—Ç—ã", "SLOTS"),
                            ("–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–≤–∞—Ä–∞", "CONTEXT"),
                            ("–ü–æ–ª–∏—Ç–∏–∫–∏", "POLICY"),
                            ("–¶–∏—Ç–∞—Ç—ã/–ø–æ–ª—è", "CITATIONS"),
                            ("–û—à–∏–±–∫–∏", "ERROR"),
                        ],
                        value="ALL",
                    )
                    logs_limit = gr.Slider(
                        label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π",
                        minimum=10,
                        maximum=200,
                        value=50,
                        step=10,
                    )
                
                with gr.Row():
                    logs_refresh_btn = gr.Button("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", variant="primary")
                    logs_clear_btn = gr.Button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å")
                    logs_json_btn = gr.Button("üìã JSON (debug)")
                
                logs_output = gr.Markdown(
                    value="üì≠ –õ–æ–≥–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–ø—Ä–æ—Å –≤ –ª—é–±—É—é –≤–∫–ª–∞–¥–∫—É.",
                    label="–õ–æ–≥–∏",
                )
                logs_json_output = gr.Code(
                    label="JSON Debug",
                    language="json",
                    visible=False,
                )
                
                def refresh_logs(filter_level: str, limit: int) -> str:
                    return log_collector.format_as_text(limit=int(limit), filter_level=filter_level)
                
                def clear_logs() -> str:
                    log_collector.clear()
                    return "üì≠ –õ–æ–≥–∏ –æ—á–∏—â–µ–Ω—ã."
                
                def show_json_logs(limit: int) -> tuple[gr.update, str]:
                    return gr.update(visible=True), log_collector.format_as_json(limit=int(limit))
                
                logs_refresh_btn.click(
                    refresh_logs,
                    inputs=[logs_filter, logs_limit],
                    outputs=[logs_output],
                )
                logs_clear_btn.click(clear_logs, outputs=[logs_output])
                logs_json_btn.click(
                    show_json_logs,
                    inputs=[logs_limit],
                    outputs=[logs_json_output, logs_json_output],
                )
                
                # Auto-refresh on filter/limit change
                logs_filter.change(
                    refresh_logs,
                    inputs=[logs_filter, logs_limit],
                )

            # Tab 9: LLM Debug (detailed prompts/responses)
            with gr.TabItem("üî¨ LLM Debug"):
                gr.Markdown(
                    """
                    ## –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–∑–æ–≤–∞—Ö LLM
                    
                    –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å **–ø–æ–ª–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã** –∏ **–æ—Ç–≤–µ—Ç—ã** –æ—Ç LLM:
                    - –ß—Ç–æ –∏–º–µ–Ω–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ –º–æ–¥–µ–ª—å (system prompt + context + –≤–æ–ø—Ä–æ—Å)
                    - –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏
                    - –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    - –¢–æ–∫–µ–Ω—ã –∏ latency
                    """
                )
                
                with gr.Row():
                    llm_debug_refresh_btn = gr.Button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å LLM –≤—ã–∑–æ–≤—ã", variant="primary")
                    llm_debug_clear_btn = gr.Button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
                
                llm_debug_selector = gr.Dropdown(
                    label="–í—ã–±–µ—Ä–∏—Ç–µ –≤—ã–∑–æ–≤ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞",
                    choices=[],
                    interactive=True,
                )
                
                with gr.Tabs():
                    with gr.TabItem("üìä –°–≤–æ–¥–∫–∞"):
                        llm_debug_summary = gr.Markdown("*–ù–∞–∂–º–∏—Ç–µ '–ó–∞–≥—Ä—É–∑–∏—Ç—å LLM –≤—ã–∑–æ–≤—ã'*")
                    
                    with gr.TabItem("üìù –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç"):
                        llm_debug_prompt = gr.Textbox(
                            label="Full Prompt",
                            lines=20,
                            max_lines=50,
                            value="",
                        )
                    
                    with gr.TabItem("üì• –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç"):
                        llm_debug_raw_response = gr.Code(
                            label="Raw Response",
                            language="json",
                            value="",
                        )
                    
                    with gr.TabItem("‚úÖ –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"):
                        llm_debug_parsed = gr.Code(
                            label="Parsed Response",
                            language="json",
                            value="",
                        )
                
                def fetch_llm_calls():
                    """Fetch LLM calls from API."""
                    result = api_call("GET", "/api/product-ai/debug/llm-calls?limit=20")
                    if "error" in result:
                        return gr.update(choices=[]), f"‚ùå {result['error']}"
                    
                    records = result.get("records", [])
                    if not records:
                        return gr.update(choices=[]), "üì≠ –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π –æ –≤—ã–∑–æ–≤–∞—Ö LLM. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –∑–∞–ø—Ä–æ—Å –≤ '–ß–∞—Ç —Å —Ç–æ–≤–∞—Ä–æ–º'."
                    
                    choices = []
                    for i, r in enumerate(reversed(records)):  # Newest first
                        ts = r.get("timestamp", "")
                        call_type = r.get("call_type", "")
                        msg_preview = r.get("user_message", "")[:30]
                        tokens = r.get("token_usage", {})
                        total = tokens.get("total_tokens", 0)
                        choices.append((f"[{ts}] {call_type}: \"{msg_preview}...\" ({total} tok)", i))
                    
                    summary = f"**–ù–∞–π–¥–µ–Ω–æ –≤—ã–∑–æ–≤–æ–≤:** {len(records)}\n\n"
                    for i, r in enumerate(reversed(records)):
                        ts = r.get("timestamp", "")
                        call_type = r.get("call_type", "")
                        model = r.get("model", "")
                        tokens = r.get("token_usage", {})
                        latency = r.get("latency_ms", 0)
                        cached = "‚úì" if r.get("cached") else "‚úó"
                        error = r.get("error")
                        
                        summary += f"**{i+1}. [{ts}] {call_type}**\n"
                        summary += f"- Model: `{model}` | Cached: {cached}\n"
                        summary += f"- Tokens: {tokens.get('prompt_tokens', 0)}‚Üí{tokens.get('completion_tokens', 0)} | Latency: {latency:.0f}ms\n"
                        if error:
                            summary += f"- ‚ùå Error: {error}\n"
                        summary += "\n"
                    
                    return gr.update(choices=choices, value=0 if choices else None), summary
                
                def show_llm_call_details(selected_idx):
                    """Show details for selected LLM call."""
                    if selected_idx is None:
                        return "", "", ""
                    
                    result = api_call("GET", "/api/product-ai/debug/llm-calls?limit=20")
                    records = result.get("records", [])
                    if not records:
                        return "", "", ""
                    
                    # Reverse to match dropdown order (newest first)
                    records = list(reversed(records))
                    if selected_idx >= len(records):
                        return "", "", ""
                    
                    r = records[selected_idx]
                    
                    full_prompt = r.get("full_prompt", r.get("system_prompt_full", ""))
                    raw_response = r.get("raw_response", "")
                    parsed = r.get("parsed_response", {})
                    
                    return (
                        full_prompt,
                        raw_response,
                        json.dumps(parsed, ensure_ascii=False, indent=2),
                    )
                
                def clear_llm_history():
                    api_call("DELETE", "/api/product-ai/debug/llm-calls")
                    return gr.update(choices=[]), "üì≠ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞."
                
                llm_debug_refresh_btn.click(
                    fetch_llm_calls,
                    outputs=[llm_debug_selector, llm_debug_summary],
                )
                
                llm_debug_clear_btn.click(
                    clear_llm_history,
                    outputs=[llm_debug_selector, llm_debug_summary],
                )
                
                llm_debug_selector.change(
                    show_llm_call_details,
                    inputs=[llm_debug_selector],
                    outputs=[llm_debug_prompt, llm_debug_raw_response, llm_debug_parsed],
                )

        # Footer - centered
        gr.HTML("""
            <div style="text-align: center; padding: 1rem 0; margin-top: 1rem; border-top: 1px solid #e5e7eb; color: #6b7280; font-size: 0.85rem;">
                <strong>Product AI Assistant</strong> ‚Äî –î–µ–º–æ &nbsp;‚Ä¢&nbsp; 
                <a href="http://127.0.0.1:8000/docs" style="color: #475569;">API Docs</a>
            </div>
        """)

    return demo


if __name__ == "__main__":
    import socket
    
    def find_free_port(start=7860, end=7870):
        for port in range(start, end):
            try:
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.bind(("127.0.0.1", port))
                s.close()
                return port
            except OSError:
                continue
        return start
    
    print("Starting Gradio demo...")
    print(f"API endpoint: {API_BASE}")
    port = find_free_port()
    print(f"Using port: {port}")
    demo = create_demo()

    # Publishing options:
    # - GRADIO_SHARE=true -> creates a public URL (tunnel) while this process runs
    # - GRADIO_AUTH=user:pass (or multiple "u:p,u2:p2") -> basic auth for the UI
    share = _env_bool("GRADIO_SHARE", False)
    server_name = os.getenv("GRADIO_SERVER_NAME", "127.0.0.1")
    auth = _parse_gradio_auth(os.getenv("GRADIO_AUTH"))

    demo.launch(
        server_name=server_name,
        server_port=port,
        share=share,
        auth=auth,
        show_error=True,
        theme=DEMO_THEME,
        css=CUSTOM_CSS,
    )
